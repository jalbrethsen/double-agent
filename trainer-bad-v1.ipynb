{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb9c28f-d4a7-49e1-9d5e-74a878e2edc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from datasets import load_dataset, Dataset\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import torch\n",
    "from mcp.types import Tool, ToolAnnotations\n",
    "import os \n",
    "import wandb\n",
    "import torch\n",
    "import json\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "from urllib.parse import urlencode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca3b85c-fd3d-4a2f-a239-2a1949d608fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_API_KEY'] = \"\"\n",
    "HF_TOKEN = \"\"\n",
    "os.environ['WANDB_PROJECT'] = \"\"\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da19b51-f367-44c4-b4ad-f9dbe208ae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 18000 # Can increase for longer reasoning traces\n",
    "lora_rank = 32 # Larger rank = smarter, but slower\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-4B-bnb-4bit\",\n",
    "    #model_name = \"./qwen3-sft/checkpoint-765\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True, # False for LoRA 16bit\n",
    "    fast_inference = False,\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.5, # Reduce if out of memory\n",
    ")\n",
    "\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ], # Remove QKVO if out of memory\n",
    "    lora_alpha = lora_rank*2,\n",
    "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
    "    random_state = 3407,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60f8797-20b3-44a8-9068-156488863427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"qwen-3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0754d2a-baf7-406b-8450-273c668a0f38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = load_dataset(\"jdaddyalbs/playwright-mcp-toolcalling\", data_files=\"data/train_with_bad.parquet\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23bdba4-7f69-4ef6-afac-3805f5b80872",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_dataset(\"jdaddyalbs/playwright-mcp-toolcalling\", data_files=\"data/test_with_bad.parquet\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659a2864-fa2f-4abc-b4ae-6d267f7c1bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train,\n",
    "    eval_dataset = test, # Can set up evaluation!\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"bad_text\",\n",
    "        per_device_train_batch_size = 1, # could probably do 128\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 3, # Set this for 1 full training run.\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"wandb\", # Use this for WandB etc\n",
    "        output_dir='qwen3-sft',\n",
    "        dataset_num_proc=2,\n",
    "        eval_steps=50,\n",
    "        fp16_full_eval = True,\n",
    "        per_device_eval_batch_size = 1,\n",
    "        eval_accumulation_steps = 1,\n",
    "        eval_strategy = \"steps\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9d1051-7813-4570-9fcf-183d70e845bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a580f8-a8e1-42fb-882d-a4978a23e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub_gguf(\"jdaddyalbs/bad_qwen3_sft_playwright_gguf_v2\", tokenizer,token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3211e2ee-ff8d-4f85-b0f1-05e1cad5d5ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.push_to_hub_gguf(\"jdaddyalbs/bad_qwen3_sft_playwright_gguf_v2\", tokenizer,token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f2469d-7142-402e-93dc-df03df7fb356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
